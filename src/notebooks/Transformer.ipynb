{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造数据\n",
    "\n",
    "以一个德语翻译英语为例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # 德语和英语的长度不要求相同\n",
    "    # encoder_input           # decoder_input        # decoder_output\n",
    "    [\"ich mochte ein bier P P P\", \"S i want a beer . E\", \"S i want a beer . E\"],\n",
    "    [\"ich mochte ein cola P P P\", \"S i want a coke . E\", \"S i want a coke . E\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 建立词库\n",
    "src_vacab = {\"P\": 0, \"ich\": 1, \"mochte\": 2, \"ein\": 3, \"bier\": 4, \"cola\": 5}\n",
    "src_token2word = {token: word for word, token in src_vacab.items()}\n",
    "\n",
    "tgt_vacab = {\n",
    "    \"P\": 0,\n",
    "    \"i\": 1,\n",
    "    \"want\": 2,\n",
    "    \"a\": 3,\n",
    "    \"beer\": 4,\n",
    "    \"coke\": 5,\n",
    "    \"S\": 6,\n",
    "    \"E\": 7,\n",
    "    \".\": 8,\n",
    "}\n",
    "tgt_token2word = {token: word for word, token in tgt_vacab.items()}\n",
    "\n",
    "SRC_VACAB_SIZE = len(src_vacab)  # 源序列词汇表大小\n",
    "TGT_VACAB_SIZE = len(tgt_vacab)  # 目标序列词汇表大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2  # Batch大小\n",
    "SEQ_LEN = 7  # 序列最大长度，不够的填充 `P`\n",
    "EMBEDDING_DIM = 512  # Token化为embedding后的大小\n",
    "HEAD_COUNT = 8  # 头的个数（多头注意力）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集，创建DataLoader\n",
    "def _make_data(sentences: list[list[str]]):\n",
    "    \"\"\"把单词序列转化为token序列\"\"\"\n",
    "    encoder_inputs, decoder_inputs, decoder_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "        enc_input = [[src_vacab[word] for word in sentences[i][0].split()]]\n",
    "        dec_input = [[tgt_vacab[word] for word in sentences[i][1].split()]]\n",
    "        dec_output = [[tgt_vacab[word] for word in sentences[i][2].split()]]\n",
    "\n",
    "        encoder_inputs.extend(enc_input)\n",
    "        decoder_inputs.extend(dec_input)\n",
    "        decoder_outputs.extend(dec_output)\n",
    "\n",
    "    return Tensor(encoder_inputs), Tensor(decoder_inputs), Tensor(decoder_outputs)\n",
    "\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(\n",
    "        self, encoder_inputs: Tensor, decoder_inputs: Tensor, decoder_outputs: Tensor\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_outputs = decoder_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encoder_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.encoder_inputs[index],\n",
    "            self.decoder_inputs[index],\n",
    "            self.decoder_outputs[index],\n",
    "        )\n",
    "\n",
    "\n",
    "_encoder_inputs, _decoder_inputs, _decoder_outputs = _make_data(sentences)\n",
    "myloader = Data.DataLoader(\n",
    "    MyDataSet(_encoder_inputs, _decoder_inputs, _decoder_outputs), BATCH_SIZE, True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Attention}(Q, K, V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "- 单纯attention函数不含有可学习的参数。\n",
    "- torch的`matmul`/`@`是支持带batch的高维tensor的，它只会把最后两个维度相乘。\n",
    "\n",
    "Dropout的使用：\n",
    "- 构造：`dropout = nn.Dropout(p)`，\n",
    "- 输入：`dropout(x)`，\n",
    "- 输出：对于x中的每个元素，都有p概率被置为0。\n",
    "\n",
    "Causal mask应该是上三角还是下三角？\n",
    "- 只要记住一点：我们的目的在于，Q只想要关注一部分的K，即前面的Q单元只能看到一部分K单元。\n",
    "- 这里的QK相乘后，attention scores的维度为`(seq_len_q, seq_len_k)`，所以mask要设置为**下三角矩阵**。\n",
    "\n",
    "整个计算完成后，得到的attention_value的维度为`(seq_len_q, embedding_dim_k)`\n",
    "理论上，K的embedding_dim不需要和QK相同，但为了简化模型，大部分模型实现中都设置为相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mha_shape_check(query: Tensor, key: Tensor, value: Tensor) -> bool:\n",
    "    \"\"\"检查MHA各个输入Tensor的形状。\n",
    "\n",
    "    Returns:\n",
    "        bool: 输入是否batched。\n",
    "    \"\"\"\n",
    "    is_batched = None\n",
    "    if query.dim() == 3:\n",
    "        is_batched = True\n",
    "        assert key.dim() == 3 and value.dim == 3, (\n",
    "            \"对于 batched 3-D `query`，`key`和`value`应为3-D，\"\n",
    "            f\"然而输入分别为{key.dim()}-D和{value.dim()}-D\"\n",
    "        )\n",
    "        assert query.shape[0] == key.shape[0] == value.shape[0], (\n",
    "            \"对于 batched 3-D `query`、`key`和`value`，批次大小（batch_size）应该相等，\"\n",
    "            f\"然而输入分别为{query.shape[0]}、{key.shape[0]}和{value.shape[0]}\"\n",
    "        )\n",
    "    elif query.dim() == 2:\n",
    "        is_batched = False\n",
    "        assert key.dim() == 2 and value.dim() == 2, (\n",
    "            \"对于 unbatched 2-D `query`，`key`和`value`应为2-D，\"\n",
    "            f\"然而输入分别为{key.dim()}-D和{value.dim()}-D\"\n",
    "        )\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            f\"`query`应为 unbatched 2-D tensor 或 batched 3-D tensor，然而输入为{query.dim()}-D\"\n",
    "        )\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1], (\n",
    "        \"`query`和`key`要求同样的特征大小（embedding_size），\"\n",
    "        f\"然而输入分别为{query.shape[-1]}和{key.shape[-1]}\"\n",
    "    )\n",
    "    assert key.shape[-2] == value.shape[-2], (\n",
    "        \"`key`和`value`要求同样的序列长度（sequence_len），\"\n",
    "        f\"然而输入分别为{key.shape[-2]}和{value.shape[-2]}\"\n",
    "    )\n",
    "\n",
    "    return is_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scaled_dot_product_attention(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    mask: Tensor | None = None,\n",
    "    dropout: nn.Dropout = None,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"计算attention\"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "    # torch的矩阵乘法支持带batch的乘法，因此二维以上的矩阵也可以相乘\n",
    "    scores = query @ key.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # mask == 0的位置都设置为负无穷\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    attention_value = scores @ value\n",
    "    return attention_value, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拆分多头\n",
    "\n",
    "把QKV的最后一维embedding_dim拆分成多个head_dim, 即投射到一个较小的维度上：\n",
    "- 原QKV形状为：`(batch_size, seq_len, embedding_dim)`\n",
    "- 拆分后形状为：`(batch_size, head_count, seq_len, head_dim)`\n",
    "\n",
    "每个头都是单独的权重矩阵。在代码的实现中，多个头是拼接在一起的，和一个大权重矩阵相乘（这个大矩阵其实就看做多个权重矩阵的拼接）。\n",
    "- 这都得益于pytorch方便的矩阵乘法，使得我们可以做到**并行计算**。\n",
    "\n",
    "### 合并多头\n",
    "\n",
    "最终，多个头的attention score拼接在一起后，还要应用一个输出权重矩阵 $W^O$ ，得到最终的输出。\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\text{MultiHead}(Q,K,V) &= \\text{Concat}(\\text{head}_1,\\cdots,\\text{head}_h)W^O \\\\\n",
    "  \\textbf{where}\\ \\text{head}_i &= \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "  \\end{align*}\n",
    "  $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_count):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_count = head_count\n",
    "        self.q_weight = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_weight = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_weight = nn.Linear(embedding_dim, embedding_dim)\n",
    "        # 输出权重矩阵W_O\n",
    "        self.output_weight = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(\n",
    "        self, q_seq: Tensor, k_seq: Tensor, v_seq: Tensor, mask: Tensor | None = None\n",
    "    ):\n",
    "        r\"\"\"进行multi-head attention的计算\"\"\"\n",
    "        is_batched = _mha_shape_check(q_seq, k_seq, v_seq)\n",
    "        # 如果QKV是2维的，增加一个batch维度\n",
    "        if is_batched is not True:\n",
    "            q_seq = q_seq.unsqueeze(0)\n",
    "            k_seq = k_seq.unsqueeze(0)\n",
    "            v_seq = v_seq.unsqueeze(0)\n",
    "\n",
    "        batch_size, src_seq_len, input_embedding_dim = q_seq.shape\n",
    "        _, tgt_seq_len, _ = k_seq.shape\n",
    "        # !这里为了简化模型，假定QKV的embedding_dim全部相等\n",
    "        if input_embedding_dim != v_seq.shape[-1]:\n",
    "            raise ValueError(\n",
    "                \"`value`的维度（embedding_dim）要求与`query`和`key`相等，\"\n",
    "                f\"而输入的`value`维度是{v_seq.shape[-1]}，`query`和`key`的维度是{input_embedding_dim}\"\n",
    "            )\n",
    "        if input_embedding_dim != self.embedding_dim:\n",
    "            raise ValueError(\n",
    "                \"输入的`query`、`key`和`value`的维度（embedding_dim）和MHA模型预设维度不匹配，\"\n",
    "                f\"模型预设维度为{self.embedding_dim}，而输入维度是{input_embedding_dim}\"\n",
    "            )\n",
    "\n",
    "        queries: Tensor = self.q_weight(q_seq)\n",
    "        keys: Tensor = self.k_weight(k_seq)\n",
    "        values: Tensor = self.v_weight(v_seq)\n",
    "        # 拆分多头\n",
    "        head_dim = input_embedding_dim // self.head_count\n",
    "        # 即最后一维拆分 -> embedding_dim = head_count * head_dim，并交换head_count和seq_dim维度\n",
    "        queries = (\n",
    "            queries.contiguous()\n",
    "            .view(batch_size, src_seq_len, self.head_count, head_dim)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        keys = (\n",
    "            keys.contiguous()\n",
    "            .view(batch_size, tgt_seq_len, self.head_count, head_dim)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        values = (\n",
    "            values.contiguous()\n",
    "            .view(batch_size, tgt_seq_len, self.head_count, head_dim)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        # 计算注意力\n",
    "        # mask = torch.tril(torch.ones(src_seq_len, src_seq_len, dtype=bool))\n",
    "        attention_values, _ = _scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        # 合并多头\n",
    "        attention_values = (\n",
    "            attention_values.permute(0, 2, 1, 3)\n",
    "            .contiguous()\n",
    "            .view(batch_size, src_seq_len, input_embedding_dim)\n",
    "        )\n",
    "        return self.output_weight(attention_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding\n",
    "\n",
    "**`nn.Embedding`中`padding_idx`参数的作用**：大部分模型都要求固定大小的输入数据，而现实中的句子不可能都是等长的，所以在把词元转化为token时，通常要多加一个token用来作为padding。而`padding_idx`的作用正是指明我们使用的padding是哪一个数，Embedding层会将其映射为0向量，并且这个位置不参与梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    \"\"\"用来将Token转化为embedding，其实就是封装了一下nn.Embedding\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, padding_token=0):\n",
    "        super().__init__(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_token,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding（位置编码）\n",
    "正弦/余弦位置编码：\n",
    "每个维度都是独特的值，偶数维度用sin，奇数维度用cos：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{PE}_{pos, 2i} &= \\sin{(pos/10000^{2i/d_{\\text{model}}})} \\\\\n",
    "\\text{PE}_{pos, 2i+1} &= \\cos{(pos/10000^{2i/d_{\\text{model}}})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "其中，$pos$代表token位置索引，$i$用来表示位置编码的维度（embedding维度）索引，$2i$表示偶数位置，$2i+1$表示奇数位置。\n",
    "\n",
    "加上位置矩阵后，同样的词组成的句子，词的语序不一样，那么词的embedding也不一样，这就成功带上了语序信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 位置编码，不参与学习\n",
    "        self.positional_encoding = torch.zeros(seq_len, embedding_dim)\n",
    "        self.positional_encoding.requires_grad_(False)\n",
    "\n",
    "        # pos和index一个列向量，一个行向量，\n",
    "        # 在计算时经过python广播，就得到了一个(seq_len, embedding_dim)的矩阵\n",
    "        pos = torch.arange(0, seq_len)\n",
    "        pos = pos.float().unsqueeze(1)\n",
    "        index = torch.arange(0, embedding_dim)\n",
    "        index = pos.float().unsqueeze(0)\n",
    "        _tmp = pos / torch.pow(10000, index / embedding_dim)\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(_tmp[:, 0::2])\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(_tmp[:, 1::2])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"接受一个(seq_len, embedding_dim)的输入，把PE加给它\"\"\"\n",
    "        input_seq_len = input.shape[-2]\n",
    "        input_embedding_dim = input.shape[-1]\n",
    "        if input_seq_len != self.seq_len or input_embedding_dim != self.embedding_dim:\n",
    "            raise ValueError(\"[PositionEmbedding] 输入维度和模块维度不匹配\")\n",
    "        return self.positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合TokenEmbedding和PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"对 TokenEmbedding 和 PositionalEncoding 的整合\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_len, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(seq_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.token_embedding(input)\n",
    "        output = output + self.positional_encoding(output)\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm\n",
    "\n",
    "Norm层的作用就归一化数据，从而保持数分布的稳定性。\n",
    "\n",
    "- **原始的transformer中，Norm层放在注意力机制之后**。这样有助于模型更好地保留和学习输入数据之间的关系。\n",
    "- **ViT中，Norm层则放在注意力机制之前**。这样有助于针对图像数据调整输入特征的尺度。\n",
    "\n",
    "Norm层的入参为`(N, L, C)`的tensor，即`(batch_size, seq_len, channel_dim)`，其中channel_dim（通道数）指的是输入数据的一个最小单元具有的维度，比如一句话的embedding_size，或者一张图片的通道数。\n",
    "\n",
    "- BatchNorm是通过对每个通道在**小批量**（batch）上计算均值和方差来进行规范化。\n",
    "$$\n",
    "  \\text{BN}(x_i)=\\gamma\\left(\\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}\\right)+\\beta\n",
    "  $$  其中， $x_i$ 是输入通道， $\\mu_B$ 是**批量上通道的均值**， $\\sigma_B^2$ 是**批量上通道的方差**， $\\gamma$ 和 $\\beta$ 都是**可学习参数**， $\\epsilon$ 代表一个很小的数，其目的是为了**避免除以零**。  \n",
    "- LayerNorm则是对**单个数据样本**的所有特征进行规范化，通常用于处理序列数据（如RNN或Transformer）。\n",
    "$$\n",
    "  \\text{LN}(x_i)=\\gamma\\left(\\frac{x_i-\\mu_L}{\\sqrt{\\sigma_L^2+\\epsilon}}\\right)+\\beta\n",
    "  $$  其中， $\\mu_L$ 是**单个数据样本所有通道的均值**， $\\sigma_L^2$ 是**单个数据样本所有通道的方差**，其他和BN一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, channel_dim, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        # 可学习参数\n",
    "        self.gamma = nn.Parameter(torch.ones(channel_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(channel_dim))\n",
    "        # epsilon\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        mean = input.mean(-1, keepdim=True)\n",
    "        var = input.var(-1, unbiased=False, keepdim=True)\n",
    "        output = (input - mean) / torch.sqrt(var + self.epsilon)\n",
    "        output = self.gamma * output + self.beta\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Netwroks\n",
    "\n",
    "就是两次Linear映射，以及一个ReLU激活函数：\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x)=\\max (0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "Feed Forward层(前馈层)具有以下几个重要作用:\n",
    "\n",
    "1. **引入非线性变换**：Feed Forward层通过ReLU等非线性激活函数，为模型引入非线性变换能力，增强模型的表达能力和学习复杂模式的能力。\n",
    "2. **特征提取和转换**：它将注意力层的输出映射到更高维的空间，然后再映射回原始维度，从而提取和转换更深层次的特征。\n",
    "3. **参数化自注意力模块**：Feed Forward层为每个自注意力块提供了独立的参数集，使得不同层的自注意力模块可以学习不同的特征和模式。\n",
    "4. **充当键值记忆**：Feed Forward层可以被视为一种键值记忆机制，其中每个\"键\"对应训练数据中的文本模式，每个\"值\"则生成输出词汇的分布。\n",
    "5. **层间信息精炼**：通过残差连接，Feed Forward层可以逐步精炼前一层的输出，逐步构建最终的输出分布。\n",
    "6. **增加模型容量**：Feed Forward层通常包含大量参数(例如BERT-base中每个Feed Forward层有3072个隐藏单元)，显著增加了模型的整体容量。\n",
    "7. **位置独立处理**：Feed Forward层对每个位置的向量进行相同的独立处理，这种并行性有助于提高计算效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.full_connection_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.full_connection_2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.full_connection_1(input)\n",
    "        output = torch.relu(output)\n",
    "        if self.dropout is not None:\n",
    "            output = self.dropout(output)\n",
    "        output = self.full_connection_2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单个Encoder块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_count, ffn_hidden_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(embedding_dim, head_count)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.norm1 = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.feedforward = PositionwiseFeedForward(\n",
    "            embedding_dim, ffn_hidden_dim, dropout_prob\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.norm2 = LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, input: Tensor, src_mask: Tensor | None = None):\n",
    "        _input = input\n",
    "        input = input + self.self_attention(input, input, input, src_mask)\n",
    "        input = self.dropout1(input)\n",
    "        input = self.norm1(_input + input)  # 一个跳跃链接\n",
    "\n",
    "        _input = input\n",
    "        input = self.feedforward(input)\n",
    "        input = self.dropout2(input)\n",
    "        input = self.norm2(_input + input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单个Decoder块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_count, ffn_hidden_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.masked_attention = MultiHeadAttention(embedding_dim, head_count)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.norm1 = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attention = MultiHeadAttention(embedding_dim, head_count)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.norm2 = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.feedforward = PositionwiseFeedForward(\n",
    "            embedding_dim, ffn_hidden_dim, dropout_prob\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        self.norm3 = LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: Tensor,\n",
    "        encoder_output: Tensor,\n",
    "        src_tgt_mask: Tensor | None = None,\n",
    "        tgt_mask: Tensor | None = None,\n",
    "    ):\n",
    "        _input = input\n",
    "        seq_len = input.shape[-2]\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=bool))\n",
    "        input = input + self.masked_attention(input, input, input, tgt_mask)\n",
    "        input = self.dropout1(input)\n",
    "        input = self.norm1(_input + input)\n",
    "\n",
    "        _input = input\n",
    "        input = input + self.cross_attention(\n",
    "            input, encoder_output, encoder_output, src_tgt_mask\n",
    "        )\n",
    "        input = self.dropout2(input)\n",
    "        input = self.norm2(_input + input)\n",
    "\n",
    "        _input = input\n",
    "        input = self.feedforward(input)\n",
    "        input = self.dropout3(input)\n",
    "        input = self.norm3(_input + input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合模块，组成Encoder和Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        src_seq_len,\n",
    "        embedding_dim,\n",
    "        head_count,\n",
    "        ffn_hidden_dim,\n",
    "        encoder_block_count,\n",
    "        dropout_prob,\n",
    "        device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = TransformerEmbedding(\n",
    "            src_vocab_size, embedding_dim, src_seq_len, dropout_prob\n",
    "        )\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            EncoderBlock(embedding_dim, head_count, ffn_hidden_dim, dropout_prob)\n",
    "            for _ in range(encoder_block_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, input, src_mask):\n",
    "        output = self.embedding(input)\n",
    "        for block in self.encoder_blocks:\n",
    "            output = block(output, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tgt_vocab_size,\n",
    "        tgt_seq_len,\n",
    "        embedding_dim,\n",
    "        head_count,\n",
    "        ffn_hidden_dim,\n",
    "        decoder_block_count,\n",
    "        dropout_prob,\n",
    "        device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = TransformerEmbedding(\n",
    "            tgt_vocab_size, embedding_dim, tgt_seq_len, dropout_prob\n",
    "        )\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            DecoderBlock(embedding_dim, head_count, ffn_hidden_dim, dropout_prob)\n",
    "            for _ in range(decoder_block_count)\n",
    "        )\n",
    "        # 最后的线性层\n",
    "        self.full_connection = nn.Linear(embedding_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, input, encoder_output, src_tgt_mask, tgt_mask):\n",
    "        output = self.embedding(input)\n",
    "        for block in self.decoder_blocks:\n",
    "            output = block(output, encoder_output, src_tgt_mask, tgt_mask)\n",
    "        output = self.full_connection(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完成Transformer\n",
    "\n",
    "重点在于构建**mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vacab_size,\n",
    "        tgt_vacab_size,\n",
    "        src_seq_len,\n",
    "        tgt_seq_len,\n",
    "        src_padding_idx,\n",
    "        tgt_padding_idx,\n",
    "        embedding_dim,\n",
    "        head_count,\n",
    "        ffn_hidden_dim,\n",
    "        encoder_block_count,\n",
    "        decoder_block_count,\n",
    "        dropout_prob,\n",
    "        device,\n",
    "    ):\n",
    "        self.encoder = Encoder(\n",
    "            src_vacab_size,\n",
    "            src_seq_len,\n",
    "            embedding_dim,\n",
    "            head_count,\n",
    "            ffn_hidden_dim,\n",
    "            encoder_block_count,\n",
    "            dropout_prob,\n",
    "            device,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            tgt_vacab_size,\n",
    "            tgt_seq_len,\n",
    "            embedding_dim,\n",
    "            head_count,\n",
    "            ffn_hidden_dim,\n",
    "            decoder_block_count,\n",
    "            dropout_prob,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.tgt_padding_idx = tgt_padding_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_padding_mask(\n",
    "        self, query: Tensor, key: Tensor, query_padding_index, key_padding_index\n",
    "    ):\n",
    "        \"\"\"生成 padding mask。在 embedding 之前使用。\n",
    "\n",
    "        想想计算注意力时，Q 乘 K 转置的结果，它是一个 `(query_seq_len, key_seq_len)` 的矩阵，也就是说，\n",
    "        横轴对应 Q，纵轴对应 K。那么，只要 Q 或者 K 其中有一者为 padding，该位置就需要 mask。\n",
    "\n",
    "        Padding mask 的形状为 `(batch_size, 1, query_seq_len, key_seq_len)`，意义是：\n",
    "        每一个句子的 padding 都不一样，所以要具体到 batch_size，而 head 是从 embedding 分出来的，\n",
    "        每一个 head 对应的句子都是一样的，所以第二个维度设置为 1，利用广播机制复制到 head_count 维上。\n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Q 矩阵，形状为 `(batch_size, query_seq_len)`\n",
    "            key (Tensor): V 矩阵，形状为 `(batch_size, key_seq_len)`\n",
    "            query_padding_index (int): Q 矩阵中，哪一个 token 代表 padding\n",
    "            key_padding_index (int): V 矩阵中，哪一个 token 代表 padding\n",
    "        \"\"\"\n",
    "        query_seq_len = query.shape[1]\n",
    "        key_seq_len = key.shape[1]\n",
    "        # 都扩展成 mask 的形状\n",
    "        # query 扩展第 1 维和第 3 维，并将第 3 维重复到 key_seq_len 的数量\n",
    "        query = query.ne(query_padding_index).unsqueeze(1).unsqueeze(3)\n",
    "        query = query.repeat(1, 1, 1, key_seq_len)\n",
    "        # query 扩展第 1 维和第 2 维，并将第 2 维重复到 query_seq_len 的数量\n",
    "        key = key.ne(key_padding_index).unsqueeze(1).unsqueeze(2)\n",
    "        key = key.repeat(1, 1, query_seq_len, 1)\n",
    "\n",
    "        mask = query & key\n",
    "        return mask\n",
    "\n",
    "    def make_causal_mask(self, query: Tensor, key: Tensor):\n",
    "        \"\"\"生成 causal mask。在 embedding 之前使用。\n",
    "\n",
    "        Causal mask 的形状为 `(query_seq_len, key_seq_len)`，和计算注意力时 Q 和 K 转置的乘积形状一样。\n",
    "        横轴对应 Q，纵轴对应 K。\n",
    "\n",
    "        Args:\n",
    "            query (Tensor): 用作 query 的句子组成的矩阵，形状为 `(batch_size, query_seq_len)`\n",
    "            key (Tensor): 用作 key 的句子组成的矩阵，形状为 `(batch_size, key_seq_len)`\n",
    "        \"\"\"\n",
    "        query_seq_len = query.shape[1]\n",
    "        key_seq_len = key.shape[1]\n",
    "        mask = torch.tril(torch.ones(query_seq_len, key_seq_len, dtype=bool)).to(\n",
    "            self.device\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # ? Encoder 处的自注意力模块，用到了 padding mask\n",
    "        src_mask = self.make_padding_mask(\n",
    "            src, src, self.src_padding_idx, self.src_padding_idx\n",
    "        )\n",
    "\n",
    "        # ? Decoder 处的自注意力模块，既有 padding mask，又有 causal mask\n",
    "        _tgt_padding_mask = self.make_padding_mask(\n",
    "            tgt, tgt, self.tgt_padding_idx, self.tgt_padding_idx\n",
    "        )\n",
    "        _tgt_causal_mask = self.make_causal_mask(tgt, tgt)\n",
    "        # padding_mask 和 causal_mask 逐元素相乘\n",
    "        tgt_mask = _tgt_padding_mask * _tgt_causal_mask\n",
    "\n",
    "        # ? Encoder 和 Decoder 的交叉注意力模块，用到了 padding mask\n",
    "        src_tgt_mask = self.make_padding_mask(\n",
    "            src, tgt, self.src_padding_idx, self.tgt_padding_idx\n",
    "        )\n",
    "\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, encoder_output, src_tgt_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
